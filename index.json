[{"content":"Quick reference for Data Analysis with Python This post is to reference to help those that are familiar with data analysis using python in Jupyter Notebook / Jupyter Lab. If you don\u0026rsquo;t have jupyter notebooks set up you will want to find another tutorial on how to install it and learn the basics of how to use it. Tutorials and references I\u0026rsquo;ve found helpful is the Data analysis with python content on Freecodecamp , and Data analysis in python on Youtube Corey Schafer on youtube . Popular python libraries for your analysis: numpy, pandas, mtaplotlib, seaborn, statsmodels, scipy, scikit-learn, bokeh(charts that are interactive).\nNumPy NumPy is a Python package for efficiently processing numeric data and work with N-dimensional arrays. Numeric processing in NumPy is much more efficient than processing in vanilla Python. Pandas and Matplotlib is built on top of NumPy. For example a number in Python is about 28 bytes but in NumPy you can specify it to only take up 8 bits or another size. NumPy is optimized for working with arrays, will store values in continuous positions in memory and take advantage of CPU processing capabilities. NumPy is great for storing numbers, dates, booleans, not good for strings and other values.\nNumPy Basics\nimport numpy as np # NumPy arrays are more performant than python arrays a = np.array([0,1,2,3,4,5,6,7,8,9]) # NumPy allows for multi-indexing a[0], a[2], a[-1] # return (0.0, 2.0, 9.0), another numpy array a[[0, 2, -2]] # return same as above a[1:9] # slicing, return array([1,2,3,4,5,6,7,8]) a[::2] # get every 2nd value return array([0,2,4,6,8]) a.dytpe # return dtype[\u0026#39;int64\u0026#39;], default is int64 a = array([1,2,3,4,5], dypte=np.int8) # indicate type when creating array a.sum() a.mean() a.std() a.var() # Work with multi-dimensional arrays, children dimensions should be the same shape A = np.array([ [0,1,2], [3,4,5], [6,7,8] ]) A[1] = np.array([10,10,10]) # will assign index 1 of A to be [10,10,10] A[2] = 99 # will assign index 2 of A to be [99,99,99], expand to fit shape A.shape A.ndim A.size A[d1:number][d2:number] # can select similar to Python arrays  A[d1:number, d2:number] # but can also select within a single square bracket A.sum() # returns sum of all values in A A.mean() A.std() A.sum(axis=0) # return sum by column A.sum(axis=1) # return sum by row # Others np.arange(10).reshape(2,5) # change shape of initialized array np.linspace(0,1,5) np.zeros((5,3), dtype=np.int) np.ones((3,3)) np.empty(10) Vectorized operations\na = np.arange(4) # array([0,1,2,3]) a+10 # array[10,11,12,13], works with subtraction(-), multiplication(*)...etc # arrays are immutable, creating new array and returning the new array.  b = np.array([5,10,15,20]) a+b # array([5,11,17,23]), adds corresponding values, shape has to be the same a[[True,False,True,False]] # selects values that are True return array([0,2]) a_filter = a\u0026gt;=2 # returns a boolean array that you can use to filter, and use and \u0026amp; / or | operators to chain conditions a[a_filter] # return array([2,3]) #Linear algebra A = np.array([ [1,2,3], [4,5,6], [7,8,9] ]) B = np.array([ [6,5], [4,3], [2,1] ]) A.dot(B) A @ B # same as A.dot(B) B.T # transpose of B Pandas You can reference the Pandas User Guide for more topics on how to use pandas\nDataframe basics import pandas as pd # Creating Series and Dataframes # Series s = pd.Series([number,number,number,number,number, ...]) s.values # return array([value, value, value, value, ...]) s.index # view the format of the index s.index.tolist() # get list of the index s.index = [\u0026#34;index_name\u0026#34;, \u0026#34;index_name\u0026#34;, \u0026#34;index_name\u0026#34;, \u0026#34;index_name\u0026#34;, ...] # can assign index to something else # Dataframes df = pd.DataFrame(data_matrix_with_rows_and_columns) df = pd.DataFrame({\u0026#34;column_name\u0026#34;: your_series, \u0026#34;column_name\u0026#34;: your_series, ...}) # create dataframe from series # See the first five rows of your dataframe, can include number as parameter to show more your_df.head() #default shows five rows your_df.head(10) #shows ten rows your_df.tail() # Selecting rows and columns,  # Selecting a single row or column returns a Series, selecing multiple rows and/or columns returns a DataFrame your_df.loc[\u0026#34;row_name\u0026#34;] # selects a specific row by name your_df.loc[\u0026#34;row_name\u0026#34;:\u0026#34;row_name\u0026#34;] # selects a range of rows by name, can also be numeric  your_df.iloc[-1] #selects a row by index, in this case it selects the last row your_df[\u0026#34;column_name\u0026#34;] # selects a specific column by name your_df[\u0026#34;column_name\u0026#34;: \u0026#34;column_name\u0026#34;] # selects a range of columns, is inclusive # Show information about your dataframe your_df.info() your_df.size your_df.describe() your_df.shape your_df.dtypes your_df[\u0026#34;column_name\u0026#34;].mean() your_df[\u0026#34;column_name\u0026#34;].median() your_df[\u0026#34;column_name\u0026#34;].max() your_df[\u0026#34;column_name\u0026#34;].min() your_df[\u0026#34;column_name\u0026#34;].unique() your_df.corr() # Get count of unique values for a column your_df[\u0026#34;column_name\u0026#34;].value_counts() Filtering and modifying # Get a Series (one column dataframe) and rows are equal to boolean values:  # True if it equals your \u0026#34;value_your_want_to_match\u0026#34;, otherwise it\u0026#39;s False filter_by = your_df[\u0026#34;column_name\u0026#34;] == \u0026#34;value_you_want_to_match\u0026#34; # can use as filter for your dataframe # Can include multiple conditions filter_by_multiple = condition_one | condition_two \u0026amp; condition_three your_df[filter_by] # only select rows where corresponding filter_by row is True for the specific \u0026#34;column_name\u0026#34; your_df[filter_by_multiple] # Select \u0026#34;column_name\u0026#34; columns where row values match filter_by your_df.loc[filter_by, \u0026#34;column_name\u0026#34;] # selects 1 column your_df.loc[filter_by, [\u0026#34;column_name\u0026#34;, \u0026#34;column_name\u0026#34;, ...]] # selects multiple columns # Select all columns specified where row values match filter_by your_df.loc[filter_by, [\u0026#34;column_name\u0026#34;, \u0026#34;column_name\u0026#34;, \u0026#34;column_name\u0026#34;, ...]] # Operations on columns, need inplace=True to persist change or assign to another variable df.drop(\u0026#34;row_name\u0026#34;) #drop a single row by name df.drop([\u0026#34;row_name\u0026#34;, \u0026#34;row_name\u0026#34;, ...]) # drop multiple rows by name df.drop(columns=[\u0026#34;column_name\u0026#34;, \u0026#34;column_name\u0026#34;, ...]) your_df.dropna(axis=\u0026#34;index\u0026#34;, how=\u0026#34;all\u0026#34;, subset=[\u0026#34;column1\u0026#34;, \u0026#34;column2\u0026#34;]) df[\u0026#34;column_name\u0026#34;].unique() // get unique values of a \u0026#34;column_name\u0026#34; column your_df[\u0026#34;column_name\u0026#34;].value_counts() // get the counts of unique values of \u0026#34;column_name\u0026#34; column your_df.rename(columns={\u0026#34;column_name\u0026#34;: \u0026#34;new_column_name\u0026#34;, \u0026#34;column_name\u0026#34;:\u0026#34;new_column_name\u0026#34;, ...} index={\u0026#34;row_name\u0026#34;:\u0026#34;new_row_name\u0026#34;, \u0026#34;row_name\u0026#34;: \u0026#34;new_row_name\u0026#34;, ...}) your_df.set_index(\u0026#34;column_name\u0026#34;, inplace=True) # sets index to values of a specified column your_df.sort_index(ascending=False) # can sort data by index # Create Series your_series = pd.Series( [\u0026#34;value\u0026#34;, \u0026#34;value\u0026#34;, \u0026#34;value\u0026#34;,...], index=[\u0026#34;row_name\u0026#34;, \u0026#34;row_name\u0026#34;, \u0026#34;row_name\u0026#34;,...], name=\u0026#34;column_name\u0026#34;) # Modfiying data in a Series your_df[\u0026#34;new_column_name\u0026#34;] = your_series # this will append your_series as a new column to your_df your_df[\u0026#34;column_name\u0026#34;] = value # change all values in column_name to value your_df.iloc[index:number] = value your_df[filter_condition] = value Cleaning and removing missing data\npd.isnull(value_or_series) # True if value is np.nan, None pd.notnull(value_or_series) pd.isna(value_or_series) # same as isnull(), preferred is isnull() pd.notna(value_or_series) pd.isnull(value_or_series).sum() #returns sum of boolean values in Series, True value is 1 and False is 0 your_df.dropna() # Check and remove duplicates your_series.duplicated() # returns a boolean array of whether value is duplicated or not, default is first your_series.duplicated(keep=\u0026#34;last\u0026#34;) #keep can be first, last, True, or False your_series.duplicated(subset=[\u0026#34;column_name\u0026#34;]) your_series.drop_duplicates() Grouping and Aggregating data grouped_df = your_df.groupby([\u0026#34;column_name\u0026#34;]) #group data by a \u0026#34;column_name\u0026#34; grouped_df.get_group(\u0026#34;value\u0026#34;) #get all the data filtered by the \u0026#34;value\u0026#34; of that column Working with different data types\nyour_df[\u0026#34;column_name\u0026#34;] = pd.to_datetime(your_df[\u0026#34;column_name\u0026#34;]) # convert column to datetime type # Look up the different methods associated with each data type.  # String methods include most of those used in Python such as split, replace, contain, ...etc your_df[\u0026#34;column_name_of_datetime_data\u0026#34;].dt.datetime_method() your_df[\u0026#34;column_name_of_string_data\u0026#34;].str.string_method() your_df[\u0026#34;column_name_of_category_data\u0026#34;].cat.category_method() Reading from a csv\n# Reading csv files your_df = pd.read_csv(\u0026#34;path_to_file.csv\u0026#34;, index_col=\u0026#34;column_name\u0026#34;, header=None, names=[\u0026#34;column_name\u0026#34;, ...], parse_dates=True) pd.set_option(\u0026#34;display.max_columns\u0026#34;, 100) pd.read_csv? # prints out all the paramaters for read_csv() pd.to_csv(\u0026#34;path_to_output.csv\u0026#34;) # Importing without pandas import csv with open(\u0026#34;file_name.csv\u0026#34;, \u0026#39;r\u0026#39;) as fp: reader = csv.reader(fp, delimiter=\u0026#34;,\u0026#34;) next(reader) for index, values in enumerate(reader): col1, col2, col3 = values print(col1,col2,col3) Reading from a database import sqlite3 # Can connect to almost all databases - mySQL, postgres, sqlite, ... etc conn = sqlite3.connect(\u0026#34;data/database_name.db\u0026#34;) your_df = pd.read_sql(\u0026#39;\u0026#39;\u0026#39;YOUR_SQL_QUERY\u0026#39;\u0026#39;\u0026#39;, conn, index_col=\u0026#34;name_of_col_for_index\u0026#34;, parse_dates=[\u0026#34;column_name_where_data_is_date\u0026#34;, \u0026#34;column_name_where_data_is_date\u0026#34;, ...]) from sqlalchemy import create_engine engine = create_engine(\u0026#34;sqlite:///database_file.db\u0026#34;) connection = engine.connect() your_df = pd.read_sql_table(\u0026#34;table_name\u0026#34;, con=connection) connection.close() your_df.to_sql? # check out how to write to the sql database Reading from an api and html import requests from IPython.core.display import display, HTML url = \u0026#34;https://address_of_your_api_and_any_query_parameters\u0026#34; data = requests.get(url, params={\u0026#34;key_name\u0026#34;: \u0026#34;value\u0026#34;, \u0026#34;key_name\u0026#34;: \u0026#34;value\u0026#34;, ...}).json() # Columns parameter is to let you name the columns for the data object your_df = pd.DataFrame(data, columns={\u0026#34;column_name\u0026#34;, \u0026#34;column_name\u0026#34;, ...}) display(HTML(url)) # where url returns a html template string your_df = pd.read_html(url) Working with Excel your_df = pd.read_excel(\u0026#34;name_of_spreadsheet.xlsx\u0026#34;) excel_file = pd.ExcelFile(\u0026#34;name_of_spreadsheet.xlsx\u0026#34;) # for parsing information  excel_file.sheet_names # see list of sheet names excel.parse(\u0026#34;sheet_name\u0026#34;) # get the dataframe from selected sheet_name your_df.to_excel() writer = pd.ExcelWriter(\u0026#34;name_of_spreadsheet.xlsx\u0026#34;) your_df.to_excel(writer, sheet_name=\u0026#34;name_of_sheet_in_spreadsheet\u0026#34;) writer.save() Plotting basics import matplotlib as plt Plotting graphs by individual column, graphs by multiple individual columns, graphs with two different columns as x and y, and graphs where data is grouped together by column value.\n# You can plot simply by calling plot method on dataframe  your_df.plot() # type_of_plot include bar, density, box, pie, hist, scatter, ... etc your_df[\u0026#34;column_name\u0026#34;].plot(kind=\u0026#34;type_of_plot\u0026#34;, vert=\u0026#34;boolean_value\u0026#34;, figsize=(width:number,height:number)) # plotting multiple columns individually plot_cols = [\u0026#34;column_name\u0026#34;, \u0026#34;column_name\u0026#34;, \u0026#34;column_name\u0026#34; ...] your_df[plot_cols].plot(kind=\u0026#34;type_of_plot\u0026#34;, subplots=\u0026#34;boolean_value\u0026#34;, layout=(col:number, row:number), figsize=(width:number, height:number)) # plotting two different columns with x and y your_df.plot(kind=\u0026#34;type_of_plot\u0026#34;, x=\u0026#34;column_name\u0026#34;, y=\u0026#34;column_name\u0026#34;, figsize=(width:number, height:number)) # plot by grouping  plot = your_df[[\u0026#34;column_name\u0026#34;, \u0026#34;column_name\u0026#34;]].type_of_plot_function(by=\u0026#34;column_name\u0026#34;, figsiz=(width:number, height:number)) plot.set_ylabel(\u0026#34;label_name\u0026#34;) plot.set_xlabel(\u0026#34;label_name\u0026#34;) # This example is graphing correlation between two columns corr = your_df.corr() plt.figure(fisize=(width:number,height:number)) plt.matshow(corr, cmap=\u0026#34;RdBu\u0026#34;, fignum=fig.number) plt.xticks(range(len(corr.columns)), corr.columns, rotation=\u0026#34;vertical\u0026#34;); plt.yticks(range(len(corr.columns)), corr.columns); # Can initialize number of rows with nrows and number of columns with ncols plt_obj = plt.subplots(nrows=number, ncols=number, figsize=(width:number,height:number)) fig, ((axes1, axes2, ...), (axes21, axes22, ...)) = plt_obj # Different types of plots, some examples below: plt.hist() plt.scatter() plt.bar() plt.boxplot() ","permalink":"https://linnali.com/posts/quick_reference_for_data_analysis_with_python/","summary":"Quick reference for Data Analysis with Python This post is to reference to help those that are familiar with data analysis using python in Jupyter Notebook / Jupyter Lab. If you don\u0026rsquo;t have jupyter notebooks set up you will want to find another tutorial on how to install it and learn the basics of how to use it. Tutorials and references I\u0026rsquo;ve found helpful is the Data analysis with python content on Freecodecamp , and Data analysis in python on Youtube Corey Schafer on youtube .","title":"Quick reference for Data Analysis with Python"},{"content":"Configure token authentication for git operations through GitHub GitHub has announced token authentication requirements for git operations . They will not support password authentication for git operations and will require using personal access tokens over HTTPS or SSH keys by August 13, 2021.\nThe recommendation is to use personal access tokens and it is explained in this youtube video about personal access tokens .\nSetting up personal access tokens You can generate the personal access token in GitHub by going into \u0026gt;Settings \u0026gt;Developer settings \u0026gt;Personal access tokens\nWhen you are prompted for your credentials during a git operation, use your personal access token instead of your password. Your personal access token will only be visible on GitHub the first time it\u0026rsquo;s generated, you will not be able to see it once you leave the page after you\u0026rsquo;ve generated it.\nYou might want to cache it on your computer so you don\u0026rsquo;t get prompted for your your email and password, or in this case your personal access token, each time. Type in the follow command in your git project directory to cache your login credentials:\n#need to type this before git asks for your login credentials usually during a push or pull request git config --global credential.helper cache Once you\u0026rsquo;ve initiated caching, start a pull or push request. For the password field, copy and paste in the token. You shouldn\u0026rsquo;t need to type your credentials in the next time you push or pull. If for some reason your computer prompts for a username and password again you can just re-generate the token or create a new token. You can change your token at any time by re-generating or deleting and creating a new one.\nYou can create multiple personal access tokens and configure them with different permissions and use/associate them for different operations. I\u0026rsquo;ve only needed the repo permissions thus far but you have the option to enable others:\nCommands and files related to this topic that might be useful:\n# see the file the stores your git configurations: cat ~/.gitconfig # edit the file the stores your git configurations (can use either vim, nano or other terminal editors): vim ~/.gitconfig #sets your git profile's email address your user.email without going into .gitconfig git config --global user.email \u0026quot;example@email.com\u0026quot; #shows what your git profile's user.name and user.email variables are set to git config -l ##deletes the cache for your credentials git config --global --unset credential.helper Setting up SSH authentication If you don\u0026rsquo;t have a ssh key or you don\u0026rsquo;t know if you do follow the guide in Generating a new ssh key and adding it to the ssh agent .\nTo generate a ssh key for github use the following command and replace the example email with your GitHub email.\nssh-keygen -t ed25519 -C \u0026quot;your_email@example.com\u0026quot; If you think you might have a ssh key, you can check the id_rsa.pub file (default location of ssh keys):\ncat ~/.ssh/id_rsa.pub Or just check the ssh directory to see what files there are\nls ~/.ssh Then follow the guide in Adding a new ssh key ot your github account to add the SSH key to your GitHub account.\n","permalink":"https://linnali.com/posts/configure_token_authentication_for_git_operations_through_github/","summary":"Configure token authentication for git operations through GitHub GitHub has announced token authentication requirements for git operations . They will not support password authentication for git operations and will require using personal access tokens over HTTPS or SSH keys by August 13, 2021.\nThe recommendation is to use personal access tokens and it is explained in this youtube video about personal access tokens .\nSetting up personal access tokens You can generate the personal access token in GitHub by going into \u0026gt;Settings \u0026gt;Developer settings \u0026gt;Personal access tokens","title":"Configure token authentication for git operations through GitHub"},{"content":"Time really flies! It\u0026rsquo;s been six weeks since I\u0026rsquo;ve become a participant at the Recurse Center (RC). It\u0026rsquo;s an amazing space for new or experienced programmers, developers, or anyone who wants to spend time learning more about code. I\u0026rsquo;ve met so many motivated and talented individuals whom have inspired me to build more and code more frequently.\nI signed up to participate from February 15th to May 7th or the \u0026ldquo;Spring 1 2021 batch\u0026rdquo; as we call it. The first couple week of RC was a bit overwhelming especially since everything is virtual at the moment and all of my interactions are via zoom and chat on RC\u0026rsquo;s zulip platform. I didn\u0026rsquo;t work on my personal projects for the first week but I did pair program on a game of life problem and did a few leetcode problems collaboratively with others RC members. This definitely opened up my perspective on how others program and also gave me reassurance that I\u0026rsquo;m learning the right things and adhering to good coding conventions. Most coding best practices are subjective and really depends on your use case.\nLearning more about Generative Art I learned more about generative art and now I\u0026rsquo;m really interested in trying to create my own works some day. I\u0026rsquo;m definitely going to learn more about p5.js. I was introduced to the works of Tyler Hobbs and really like how he tries to imitate the qualities of traditional mediums like watercolor. The colors he chooses are energetic but at the same time, calming. I would love to hang one up in my house in the future.\nChanging my website to Hugo blog I also got inspired to transform this website (linnali.com) from a html,css,javascript portfolio website to a hugo blog. I considered hugo prior to RC but I decided not to use it because it looked too plain but after seeing a fellow RC\u0026rsquo;er hugo blog I decided to convert. I liked the minimalism of it but you can add some customizations after you\u0026rsquo;re familiar with the theme structure. The important thing is that it\u0026rsquo;s a good place to start if you want to write blog posts and publish fast. I got caught up in designing my site a little more so it took longer. I watched Mike Dane\u0026rsquo;s Hugo Tutorial Series to get a basic understanding of how themes work and how to deploy. I will likely migrate this site to be a Next.js blog but for now hugo is a great template.\nAthena flights Next.js project For the past two weeks I\u0026rsquo;ve been working on Athena flights (you can find it on my Software Projects page.) It\u0026rsquo;s been a great learning experience. I was new to Next.js, Typescript, GraphQL, and Postgres so this project was a great way for me to start working with these technologies with another RC\u0026rsquo;er who has built another project with this tech stack. From this project I realized that having someone walk though a piece of code you\u0026rsquo;re working is immensely more helpful and speeds up your learning way more than just watching tutorials on the topic.\nRC is just a great place to learn from others even if they are on topics you\u0026rsquo;re not ever going to dig into. Just be careful to stay on track and prioritize your own personal goals before you get enticed down rabbit holes of other people\u0026rsquo;s passion projects. You can find people working in a variety of different programming languages (C, Haskell, Python, Javascript/Typescript, Java, Lisp, Rust, Scala, \u0026hellip;etc). There\u0026rsquo;s presentations every friday for those who want to share their work and I\u0026rsquo;ve seen amazing project from web apps and games to bots and a small scale database.\nIf this sounds like a group you\u0026rsquo;d like to be a part of you can apply at RC\u0026#39;s application page . It\u0026rsquo;s a great time. It\u0026rsquo;s completely free and since it\u0026rsquo;s all virtual you will not have to pay for NYC rent. Usually the program is held in-person and you\u0026rsquo;d have to either live or relocate to NYC but at the moment it is open to everyone around the world!\nI still have six more weeks in of my batch left and I will use this time to work on more coding project with my fellow RC members and learn more programming through zoom coffee chats and pair programming. But after my batch ends I\u0026rsquo;ll still keep learning and collaborate with others because our motto is to \u0026ldquo;Never Graduate\u0026rdquo;!\n","permalink":"https://linnali.com/posts/my-journey-as-a-participant-with-recurse-center-part-one-of-two/","summary":"Time really flies! It\u0026rsquo;s been six weeks since I\u0026rsquo;ve become a participant at the Recurse Center (RC). It\u0026rsquo;s an amazing space for new or experienced programmers, developers, or anyone who wants to spend time learning more about code. I\u0026rsquo;ve met so many motivated and talented individuals whom have inspired me to build more and code more frequently.\nI signed up to participate from February 15th to May 7th or the \u0026ldquo;Spring 1 2021 batch\u0026rdquo; as we call it.","title":"My journey as a participant with Recurse Center (Part One of Two)"},{"content":"Athena flights I worked on this project with @Joseph Tran . This is a web app built with Next.js, React and Typescript on the front-end. We used Postgres for the database and Typeorm with Apollo Server and Graphql on the back-end. We queried SpaceX\u0026rsquo;s api, then added flight information to the launches so users can choose flights to book based on destination and departure dates. I worked on both the front-end and back-end functionality to filter flights, authenticate users, cancel bookings.\nLive: athena-flights.vercel.app GitHub: athena-flights repo  Tdplaylist This is a web app I built with the MERN stack (MongoDB, Express, React and Nodejs). Authentication uses JWT. I haven\u0026rsquo;t found a task management app that lets me keep track of the time I\u0026rsquo;ve spent working on a task and since I was learning more about React I thought I\u0026rsquo;d just build one.\nLive: tdplaylist.xyz GitHub: tdplaylist repo  Clean slate I\u0026rsquo;m volunteering with CodeforBoston and working as part of Clean Slate\u0026rsquo;s data team. Our work is to analyze court data so policy makers can make it easier for under-served people expunge their criminal records.\nI help with cleaning up court data with python / pandas in Jupyter notebooks. I\u0026rsquo;m also tasked with helping with documentation and organizing files between GitHub and GDrive so new member onboarding is smoother.\nGitHub: clean-slate repo  Tetris with grids Tetris game built with vanilla javascript and css grid. Movement of tetris pieces is done by adding and removing css classes. Uses DOM EventListeners to control tetris pieces.\nLive: linnali.com/tetris-with-grid GitHub: tetris-with-grid repo ","permalink":"https://linnali.com/posts/software-projects/","summary":"Athena flights I worked on this project with @Joseph Tran . This is a web app built with Next.js, React and Typescript on the front-end. We used Postgres for the database and Typeorm with Apollo Server and Graphql on the back-end. We queried SpaceX\u0026rsquo;s api, then added flight information to the launches so users can choose flights to book based on destination and departure dates. I worked on both the front-end and back-end functionality to filter flights, authenticate users, cancel bookings.","title":"Software projects"},{"content":"I\u0026rsquo;m working on getting this hugo blog up and running. I want to have a place where I can share my thoughts on projects I\u0026rsquo;ve been working on. I\u0026rsquo;ve been keeping a google doc journal for myself but I feel like it\u0026rsquo;s a hassle to have to go online to write it. I was thinking about moving my journal to markdown anyways so I might as well just host it online with hugo as well. I\u0026rsquo;ll try this for a bit and also keep my google doc journal and see which one I like better.\nThis has been a great resource: Markdown Guide I think the problem with writing this straight in a markdown file is that there isn\u0026rsquo;t a spellchecker. My spelling have been getting progressively worse since I\u0026rsquo;ve stopped writing book reports with pen and paper.\nSpell Checker in VSCode editor Actually I just found a the solution to the problem above. For context, I\u0026rsquo;m writing this in vscode on my computer and I found that there is a spellchecker extension called \u0026ldquo;Code Spell Checker\u0026rdquo; by Street Side Software after a quick search. It works great now! I think it only checks for spelling and not grammar\u0026hellip; but google docs and microsoft word doesn\u0026rsquo;t do a spectacular job at that anyways.\nFormatting text and emojis in VSCode with keyboard shortcuts Also apparently you can press \u0026raquo; ctrl + p and type \u0026raquo; \u0026gt;join lines to format the text (probably can also add this as a keyboard shortcut. I think I\u0026rsquo;ll use leave it for now). I found that you have to disable the \u0026ldquo;Prettier\u0026rdquo; extension with Hugo otherwise it will mess up the formatting of the code parts if you go and edit the themes. There\u0026rsquo;s probably a setting I can change to have it be compatible. I\u0026rsquo;ll look into that later. Also, you can press \u0026raquo; ctrl + i to get emojis: üòÅ. Theres also Shortcodes in Hugo for that\u0026hellip; I think\u0026hellip; but I don\u0026rsquo;t know how to do that yet so this is a nice alternative. I got my info from this stackoverflow page .\nOpen external links in new tab (target=\u0026rdquo;_blank\u0026rdquo;) with markdown in Hugo Add the following code to /layout/_default/_markup/render-link.html\n\u0026lt;a href=\u0026quot;{{ .Destination | safeURL }}\u0026quot;{{ with .Title}} title=\u0026quot;{{ . }}\u0026quot;{{ end }}{{ if strings.HasPrefix .Destination \u0026quot;http\u0026quot; }} target=\u0026quot;_blank\u0026quot;{{ end }}\u0026gt;{{ .Text }}\u0026lt;/a\u0026gt; Build and deploy my Hugo blog with GitHub pages To build it all I have to do is type hugo in the directory of the project on the terminal and I will get a generated folder named \u0026ldquo;public\u0026rdquo;\nhugo I\u0026rsquo;m using GitHub pages to host my website and thus have a repo named linnal86.github.io that stores all the contents of this website. In the settings of this repo I have the custom domain set to linnali.com.\nEach time I update my website I copy and past the contents from the generated \u0026ldquo;public\u0026rdquo; folder into my local linnal86.github.io folder. To push my local updates to GitHub I have to overwrite the contents of the remote repo because it conflicts with what\u0026rsquo;s currently there. To overwrite use the following git command:\ngit push -f Once my remote repo is updated I have to go to the repo\u0026rsquo;s setting to update the custom domain again because for some reason the overwrite clears the custom domain.\n","permalink":"https://linnali.com/posts/setting-up-my-hugo-blog/","summary":"I\u0026rsquo;m working on getting this hugo blog up and running. I want to have a place where I can share my thoughts on projects I\u0026rsquo;ve been working on. I\u0026rsquo;ve been keeping a google doc journal for myself but I feel like it\u0026rsquo;s a hassle to have to go online to write it. I was thinking about moving my journal to markdown anyways so I might as well just host it online with hugo as well.","title":"Setting up my Hugo blog"},{"content":"It took me 2 weeks to figure out how to deploy my fullstack nodejs react app. It uses the MERN stack - mongodb, express, react, and nodejs. I watched multiple youtube videos and read various blogs. I had to reference numerous different sources so here is a consolidated guide of how to do it.\nDigital Ocean Setup I hosted my server on digitalocean. The alternatives includes Amazon AWS, Google Cloud, Siteground, Heroku, etc but I found that digitalocean was the easiest, most reliable and the cheapest in the long run.\nCreate Droplet on Digital Ocean Create a Digital Ocean account and then create a droplet. Choose the latest ubuntu image from Distrubutions, choose the plan and monthly rate that fits your needs. For toy apps or hobby projects you can just choose the basic plan and the cheapest monthly rate. You can always upgrade later.\nA droplet is a virtual private server (VPS).\nI DID NOT select mongodb from the marketplace. For some reason it works better for me if I manually install it on the droplet.\nChoose ubuntu, basic plan, and lowest monthly option You can choose either SSH keys or password. The password option is the fastest. Just create a password and that will be what you\u0026rsquo;ll use to login in when you ssh into the vps ubuntu linux distribution. You can change it to ssh later on Adding a domain to your application. if you want to get a SSL certificate from LetsEncrypt you will need a domain name. LetsEncrypt does not give certificates to raw ip address. You may be able to find other certificate providers who will give certificates for raw ips but I don\u0026rsquo;t know which ones\nOn digitalocean, on the sidebar go to \u0026gt; Networking Enter yourdomainname.com and add Add an A record for @ and www to your droplet.\n Add @ under HOSTNAME, select your droplet under WILL DIRECT TO and click Create Record Add www under HOSTNAME, select your droplet under WILL DIRECT TO and click Create Record  Go to domain registrar, (I use Namecheap), add the nameservers to the custom DNS section\nThis will take anywhere between 15 - 20 minutes to propogate. It may take longer. Check https://www.whatsmydns.net/ to see propgation status of your domain name.\nSSH into droplet and install mongodb, node, nginx and pm2 SSH into the vps ubuntu linux distribution by opening your terminal if your on mac or linux with the ip address generated by digitalocean. This can be found on the droplet next to the name of the droplet\nssh root@your-ip-address-from-digitalocean-next-to-droplet Enter y when terminal asks\nAre you sure you want to conitnue connecting (yes/no/[fingerprint])?\nThen it will ask you for your the root password you created in the Authentication section.\nNow you\u0026rsquo;re logged in.\nI installed mongodb manually and did not install it from the marketplace.\nsudo apt update sudo apt upgrade sudo apt install mongodb sudo systemctl status mongodb Install Node/NPM. Installing node will also install NPM in most cases. if not then also install npm Here is a link to the digitalocean node install guide for your reference\ncurl -sL https://deb.nodesource.com/setup_12.x | sudo -E bash - sudo apt install nodejs sudo apt install build-essential #if you do not have npm after installing node npm install npm@latest -g #check version node -v npm -v Install NGINX, a opensource web server. NGINX can be used as a reverse proxy, it connects server to the internet and provides protection between requests.\n#option 1 for installing nginx sudo apt install nginx #option 2 for installing nginx sudo apt-get install nginx -y Add SSL with LetsEncrypt To get commands got to Certbot website and click on Get Certbot instructions. Select the software and system to get the instructions you need to run on the droplet.\nsudo snap install core; sudo snap refresh core sudo snap install --classic certbot sudo ln -s /snap/bin/certbot /usr/bin/certbot # recommended option sudo certbot --nginx # Y to Terms of Service to register with ACEM server # Y or N to share eamil with Electronic Frontier Foundation # domainname.com to No names wer found in your configuration files. Please enter in your domain name(s) # conservative option, if you want to make changes to nginx configuration by hand sudo certbot certonly --nginx # auto renew certificates before they expire sudo certbot renew --dry-run Go to your domainname.com and you should see the default nginx page\nUpdate NGINX configuration file to direct traffic #update nginx configuration file using nano, vim, or another editor sudo nano /etc/nginx/sites-available/default Copy in the following into the file you just opened. update the values for domainname.com to be your own and port to the port you want to run on.\nCopy into first server object and comment out\n listen 80 default_server; listen [::]:80 default_server;  MAKE SURE to copy into second server object also\nThe location /api is for routing calls to the api to the node server For this example I\u0026rsquo;m using localhost:5000 (port 5000) for my frontend and localhost:5001 (port 5001) for my backend\nserver { root /var/www/html; server_name domainname.com; location / { proxy_pass http://localhost:5000; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $host; proxy_set_header X-NginX-Proxy true; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection ‚Äúupgrade‚Äù; proxy_max_temp_file_size 0; proxy_redirect off; proxy_read_timeout 240s; } location /api { proxy_pass http://localhost:5001; } } NGINX works with Ubuntu firewall ufw. type ufw app list to see options:\n Nginx-Full‚Ää‚Äî‚ÄäEnables both ports 22 and 88 for HTTP and HTTPS access, respectively. Nginx-HTTP‚Ää‚Äî‚ÄäEnables port 22 for HTTP only. Nginx-HTTPS‚Ää‚Äî‚ÄäEnables port 80 \u0026amp; HTTPS only. OpenSSH  More info on how to tell what is listing on a tcpip port ,¬†and redirect http to https in nginx .\n# Enable https ports for nginx on ubuntu firewall sudo ufw enable ufw allow 'Nginx HTTPS' ufw status # checks status of firewall #check if port is being served #IMPORTANT to make sure port 80 is being served nc -zvw3 localhost 80 #change ufw to allow a port sudo ufw allow PORT_NUMBER sudo ufw allow 80 #change ufw to delete rules by number sudo ufw status numbered sudo ufw delete RULE_NUMBER #change ufw to delete an allowed rule sudo ufw delete RULE_NAME sudo ufw delete allow 80 # Check NGINX config sudo nginx -t # Restart NGINX #option 1 to restart nginx sudo service nginx restart #option 2 to restart nginx sudo systemctl restart nginx #other nginx commands sudo systemctl status nginx sudo systemctl stop nginx Create a Hello.js Application to Test Nginx configuration cd ~ nano hello.js const http = require('http'); const hostname = 'localhost'; const port = 5000; const server = http.createServer((req, res) =\u0026gt; { res.statusCode = 200; res.setHeader('Content-Type', 'text/plain'); res.end('Hello World!\\n'); }); server.listen(port, hostname, () =\u0026gt; { console.log(`Server running at http://${hostname}:${port}/`); }); node hello.js Go to your domainname.com and you should see\n Hello World\n Install PM2 and Serve   PM2 is a process manager for nodejs applications with built-in load balancer. it keeps the app running in the background\n  Serve is for the React frontend app\n  sudo npm install pm2 -g sudo npm i -g serve #you can test out pm2 with hello.js pm2 start hello.js #you can see the status of pm2 using pm2 ps # Other pm2 commands pm2 show app.js pm2 status pm2 restart app.js pm2 stop app.js pm2 logs (Show log stream) pm2 flush (Clear logs) # To make sure app starts when reboot pm2 startup ubuntu Add the frontend React app Change your apiUrl to a realtive path, preferrably just \u0026ldquo;/api\u0026rdquo; This will make it so that your apiEndpoint will be /api/user for the user route.\n# An example will be that your React app will request either axios.post(\u0026quot;/api/users\u0026quot;, userVariable) # or it will be the following if you create wrap axios in a httpService http.post(\u0026quot;/api/users\u0026quot;, userVariable) Add the a proxy variable under the eslintConfig in package.json so that in the dev environment the apiUrl is still routing correctly\n \u0026quot;eslintConfig\u0026quot;: { \u0026quot;extends\u0026quot;: \u0026quot;react-app\u0026quot; }, \u0026quot;proxy\u0026quot;: { \u0026quot;/api\u0026quot;: { \u0026quot;target\u0026quot;: \u0026quot;http://localhost:3900\u0026quot; } }, Build your react application and move it to your droplet ubuntu linux distribution server\nnpm run build #check ufw status, if enabled you have to disable temporarily sudo ufw disable #option 1 for moving build folder into droplet sudo rsync -azv build root@droplet-ip-address:/var/www/html/ #if you want to move it into the home folder sudo rsync -azv build root@droplet-ip-address:~ #option 2 for moving build folder into droplet sudo scp -r build/ root@droplet-ip-address:/var/www/html/ #after moving the files you can enable again sudo ufw enable # try the follow first to serve frontend to see if it works serve -s /var/www/html/build/ # pm2 to serve continuously pm2 start serve -- -s /var/www/html/build/ Set up nodejs backend Clone your backend from Github and start your app to make sure it\u0026rsquo;s working\ngit clone your-backend-repo-name.git cd your-backend-repo-name npm install May need to set up environment variable if you have them. Create your relevant env, toml, json files for environment variable In your repo folder create a .env file if you\u0026rsquo;re using dot-env.\ntouch .env nano .env # copy-paste in your environment variables. Run the backend to see if it works\nnode index.js # haven't configured a script for running the app npm start # if you've set up a start script ctrl-C # to stop app Your application should run. Make sure it is running before moving on to the next step. Go to your https://domainname.com/api. Page should display Cannot GET /api\n#run the backend with pm2 to have it run in the background sudo pm2 start index.js Conclusion # To make sure app starts when reboot pm2 startup ubuntu There are a lot of different ways you can host your fullstack, frontend, backend applications and I choose put frontend and backend on a single droplet on digital ocean. If there is anything confusing or I got something wrong please let me know. Please also let me know if you find this helpful. Feel free to leave your comments in my corresponding gist\u0026amp;rsquo;s comment section . Thanks!\nSources:\nYoutube: Full Node.js Deployment - NGINX, SSL with Lets Encrypt Github Gist: Nodejs deployment PM2, NGINX, LetsEncrypt Youtube: Plan, Code, Deploy a Startup in 2 Hours ~ deploy instructions @ 1:35:00 Hackernoon: Finish deploying react app on digitalocean Digital Ocean: Set Up nodejs application Digital Ocean: Run nodejs server with Nginx Medium: Deploy fullstack with NGINX Digitalocean Dev.to: Fullstack using React, Strapi, NGINX, MongoDB on Digitalocean Configure nginx for nodejs backend and React frontend Freecodecamp: deploy first fullstack web app ","permalink":"https://linnali.com/posts/deploy-fullstack-nodejs-mongodb-express-backend-and-react-frontend-on-digital-ocean-with-nginx-single-droplet/","summary":"It took me 2 weeks to figure out how to deploy my fullstack nodejs react app. It uses the MERN stack - mongodb, express, react, and nodejs. I watched multiple youtube videos and read various blogs. I had to reference numerous different sources so here is a consolidated guide of how to do it.\nDigital Ocean Setup I hosted my server on digitalocean. The alternatives includes Amazon AWS, Google Cloud, Siteground, Heroku, etc but I found that digitalocean was the easiest, most reliable and the cheapest in the long run.","title":"Deploy fullstack Nodejs MongoDB Express backend and React frontend on Digital Ocean with NGINX single droplet"}]